# -*- coding: utf-8 -*-
"""project03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mNWDybYs1l11bJz6FeX7-YkZleYVM-80

# LAB 3 - Data cleaning

### Introduction
Welcome to the third project, which will be devoted to data cleaning.
As before, they will require that you load a dataset and process it as required, saving the indicated
file. This time, we will focus on some common data cleaning operations like concatenating, joining,
reshaping and transforming data.
If not said otherwise, maintain the original order of records when performing these operations.

### Exercise 1: Load data
Load 3 files into one big DataFramn with an index containing a consistents equence of
numbers, e.g. 0, 1, 2, â€¦. Keep the original column structure.  Use the default orient used by to_json(),
i.e. columns.
"""

import pandas as pd
import numpy as np

df1 = pd.read_json("proj3_data1.json")
df2 = pd.read_json("proj3_data2.json")
df3 = pd.read_json("proj3_data3.json")

df = pd.concat([df1, df2, df3], ignore_index=True)
df.to_json('proj3_ex01_all_data.json')
df

"""### Exercise 2: Missing values

Produce a CSV file containing column names and the number of missing values in these columns.
Only include the columns which have any missing values
"""

df_missing = pd.DataFrame(columns=["column_name", "missing_val_count"])

# Function is null returns boolean series. So when we sum we get number of True values (true = 1)
i = 0
for col in df.columns :
  missing_val = df[col].isna().sum()
  if(missing_val != 0):
    df_missing.loc[i] = [col, missing_val]
    i +=1


# Saving to csv file without index and header
df_missing.to_csv('proj3_ex02_no_nulls.csv', index=False, header=False)
df_missing

"""### Exercise 3: Applying functions"""

# load json into dictionary
import json
with open('proj3_params.json', 'r') as file:
  params = json.load(file)
params

# adding a empty (Nan values) column to DataFrame
df['description'] = np.nan

# For each row we apply function which returns list of strings in columns mentioned in params['concat_columns'] dictionary
# Join() makes space a separator and axis=1 tells function to apply on every row (defoult axis=0 so function apply to every column)
df['description'] = df.apply(lambda row: ' '.join(([str(row[col]) for col in params['concat_columns']])), axis=1)

# Saving to file
df.to_json('proj3_ex03_descriptions.json')
df

"""### Exercise 4: Joining datasets"""

df_4 = pd.read_json("proj3_more_data.json")
df_4

# we use 'join_coulmn' as index to join two dataframes (left join)
df_join = df.join(df_4.set_index(params['join_column']), on=params['join_column'], how='left')
df_join

df_join.to_json("proj3_ex04_joined.json")

"""### Exercise 5: Iterating over DataFrame"""

# iterating row-by-row and saving every row to json file
df_5 = df_join.copy()

for i, row in df_5.iterrows():
  description = row['description'].lower().replace(' ', '_')
  row.drop("description")
  row.to_json(f"proj3_ex05_{description}.json")

# similar thing but we make sure that columns in the int_columns parameter are saved as integers, not floats
# Change the columns type
for column in params['int_columns']:
  df_5[column] = df_5[column].astype('Int64') # Type int

# Replace Nan values to null (None)
df_5 = df_5.replace({np.nan : None})

for i, row in df_5.iterrows():
  description = row['description'].lower().replace(' ', '_')
  row.drop("description")
  row.to_json(f"proj3_ex05_int_{description}.json")
df_5

"""### Exercise 6: Aggregation"""

import json
df_6 = df_join.copy()
result = {}
for i, agg in enumerate(params['aggregations']):
  result[agg[1] + '_' + agg[0]] = float(df_6.agg({agg[0] : agg[1]}).iloc[0])  #agg() returns DF or Series so and we only want value so iloc() gets it
  # type changed to float instead of np.float64 (IDK if neccesarry)

with open('proj3_ex06_aggregations.json', 'w') as file:
  json.dump(result, file)

"""### Exercise 7: Grouping"""

grouped_data = df_join.groupby(params['grouping_column'])
numerical_cols = df_join.select_dtypes(include=np.number).columns   # Only include numerical columns
mean_values = grouped_data[numerical_cols].mean()  # Calculate mean of numerical columns
size_group = df_join.groupby(params['grouping_column']).size()

# get indexes of rows to we want to save
rows_to_save = []
for i, count in enumerate(size_group):
  if(count > 1):
    rows_to_save.append(i)


mean_values.iloc[rows_to_save].to_csv("proj3_ex07_groups.csv", index=True, header=True)
mean_values.iloc[rows_to_save]

"""### Exercise 8: Reshaping data"""

print(params['pivot_columns'])
print(params['pivot_index'])
print(params['pivot_values'])

df_8 = df_join.copy()
#   we use pivot_table() with agg function max to select maximum value
df_pivot_1 = df_8.pivot_table(index=params['pivot_index'], columns=params['pivot_columns'], values=params['pivot_values'], aggfunc='max')
df_pivot_1.to_pickle("proj3_ex08_pivot.pkl")

# Now we can 'unpivote' our data frame back to 'long' format
df_melted = df_8.melt(id_vars=params['id_vars'])
df_melted.to_csv('proj3_ex08_melt.csv',header=True, index=False)

df_stats = pd.read_csv('proj3_statistics.csv')
df_stats

first_col_name = df_stats.columns[0]
# melt() funciton unpivotes
df_melt = df_stats.melt(id_vars=[first_col_name], var_name="var", value_name="value")
df_melt[["prefix", "sufix"]] = df_melt["var"].str.split("_", expand=True) #expand=True -> dataFrame

first_col_vals = df_stats[first_col_name].unique()
print(first_col_vals)

# first version of df pivot
df_pivot = df_melt.pivot_table(index=[first_col_name, "sufix"], columns="prefix", values="value")
df_pivot

# reseting 'prefix', 'sufix' names
df_pivot.index.names = [None, None]
df_pivot.columns.name = None
# Level 0 means that df is getting sorted by only index[0] (first index in multiindex)
df_pivot = df_pivot.reindex(first_col_vals, level=0)
df_pivot.to_pickle("proj3_ex08_stats.pkl")
df_pivot